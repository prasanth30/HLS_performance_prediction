{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_CFG:\n",
    "    # Embedding\n",
    "    key_embed_size = 512\n",
    "    val_embed_size = 256\n",
    "    embed_size = 768 # Scale By Value\n",
    "    # Encoder\n",
    "    dim_input = 768\n",
    "    n_heads = 8\n",
    "    hidden_size = 512\n",
    "    num_inds = 32\n",
    "    num_outputs = 7\n",
    "    n_class = 1\n",
    "    ln=False\n",
    "    # Training\n",
    "    lr = 1e-5\n",
    "    n_epochs = 5\n",
    "    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "    regression_weight = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import rich\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from get_data import train_dataset, test_dataset\n",
    "from utils import collate_fn_train, collate_fn_test\n",
    "import networkx as nx\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.append('/home/karakanaidu/experiment/chip/Code/Models/Set_Transformer/set_transformer')\n",
    "from modules import MAB, SAB, ISAB, PMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pragmas:\n",
    "    pragmas = ['__PARA__L0', '__PARA__L0_0', \n",
    "               '__PARA__L0_1', '__PARA__L0_1_0', \n",
    "               '__PARA__L0_2', '__PARA__L0_2_0', \n",
    "               '__PARA__L0_3', '__PARA__L0_3_0', \n",
    "               '__PARA__L1', '__PARA__L2', '__PARA__L3', '__PARA__L4', '__PARA__L5', '__PARA__L6', \n",
    "               '__PARA__L7', '__PARA__L7_0', \n",
    "               '__PARA__L8', \n",
    "               '__PIPE__L0', '__PIPE__L0_1', '__PIPE__L0_2', '__PIPE__L0_3', \n",
    "               '__PIPE__L1', '__PIPE__L2', '__PIPE__L3', '__PIPE__L4', '__PIPE__L5', '__PIPE__L7', \n",
    "               '__TILE__L0', '__TILE__L0_1', '__TILE__L0_2', '__TILE__L0_3', \n",
    "               '__TILE__L1', '__TILE__L2', '__TILE__L3', '__TILE__L4', '__TILE__L5']\n",
    "    categorical_space = [\n",
    "                '__PIPE__L0','__PIPE__L1','__PIPE__L2','__PIPE__L3',\n",
    "                '__PIPE__L0_1','__PIPE__L0_2','__PIPE__L0_3',\n",
    "                '__PIPE__L4','__PIPE__L5','__PIPE__L7',\n",
    "    ]\n",
    "    integer_space = [\n",
    "                '__PARA__L0', \n",
    "                '__PARA__L0_0', '__PARA__L0_1', '__PARA__L0_1_0', \n",
    "                '__PARA__L0_2', '__PARA__L0_2_0', '__PARA__L0_3', '__PARA__L0_3_0', \n",
    "                '__PARA__L1', '__PARA__L2', '__PARA__L3', '__PARA__L4', '__PARA__L5', '__PARA__L6', '__PARA__L7', \n",
    "                '__PARA__L7_0', '__PARA__L8', \n",
    "                '__TILE__L0', '__TILE__L0_1', '__TILE__L0_2', '__TILE__L0_3', \n",
    "                '__TILE__L1', '__TILE__L2', '__TILE__L3', '__TILE__L4', '__TILE__L5'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './../../Data/train_data/data/designs/v18'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_ds \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m val_ds \u001b[38;5;241m=\u001b[39m train_dataset(split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m test_ds \u001b[38;5;241m=\u001b[39m test_dataset()\n",
      "File \u001b[0;32m~/experiment/chip/Code/get_data.py:40\u001b[0m, in \u001b[0;36mtrain_dataset.__init__\u001b[0;34m(self, versions, split, test_size, random_state, filter)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state \u001b[38;5;241m=\u001b[39m random_state  \u001b[38;5;66;03m# Seed for reproducibility\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfilter\u001b[39m\n\u001b[0;32m---> 40\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/experiment/chip/Code/get_data.py:70\u001b[0m, in \u001b[0;36mtrain_dataset.generate_array\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m version \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mversions:\n\u001b[1;32m     69\u001b[0m     design_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(base_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_data\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdesigns\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mversion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m fname \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdesign_path\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     71\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m fname:\n\u001b[1;32m     72\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './../../Data/train_data/data/designs/v18'"
     ]
    }
   ],
   "source": [
    "train_ds = train_dataset(split='train')\n",
    "val_ds = train_dataset(split='val')\n",
    "test_ds = test_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_ds, batch_size=1, collate_fn = collate_fn_train)\n",
    "val_dataloader = DataLoader(val_ds, batch_size=1, shuffle=True, collate_fn = collate_fn_train)\n",
    "test_dataloader = DataLoader(test_ds, batch_size=1, shuffle=False, collate_fn = collate_fn_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class set_transformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(set_transformer, self).__init__()\n",
    "        self.enc = nn.Sequential(\n",
    "                        SAB(model_CFG.dim_input,   model_CFG.hidden_size, model_CFG.n_heads, ln=model_CFG.ln),\n",
    "                        SAB(model_CFG.hidden_size, model_CFG.hidden_size, model_CFG.n_heads, ln=model_CFG.ln),\n",
    "                        # SAB(model_CFG.hidden_size, model_CFG.hidden_size, model_CFG.n_heads, ln=model_CFG.ln),\n",
    "                    )\n",
    "        self.dec = nn.Sequential(\n",
    "                        PMA(model_CFG.hidden_size, model_CFG.n_heads, model_CFG.num_outputs, ln=model_CFG.ln),\n",
    "                        # SAB(model_CFG.hidden_size, model_CFG.hidden_size, model_CFG.n_heads, ln=model_CFG.ln),\n",
    "                        # SAB(hidden_size, hidden_size, num_heads, ln=ln),\n",
    "                        nn.Linear(model_CFG.hidden_size, model_CFG.n_class)\n",
    "                    )\n",
    "\n",
    "        self.enc = self.enc.to(device=model_CFG.device)\n",
    "        self.dec = self.dec.to(device=model_CFG.device)\n",
    "        \n",
    "        no_of_embeddings = len(pragmas.categorical_space*3 + pragmas.integer_space)\n",
    "        \n",
    "        cat_table = {f'{label}_{value}' : self.kaiming_init_embedding(model_CFG.embed_size) for label in pragmas.categorical_space for value in ['','off','flatten']}\n",
    "        \n",
    "        reg_table = {label : self.kaiming_init_embedding(model_CFG.key_embed_size) for label in pragmas.integer_space}\n",
    "        self.reg_value_encoder = nn.ModuleDict({label : nn.Linear(1,model_CFG.val_embed_size) for label in pragmas.integer_space})\n",
    "        \n",
    "        self.embeddings = nn.ParameterDict(cat_table | reg_table)\n",
    "        \n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = True\n",
    "    def kaiming_init_embedding(self, size):\n",
    "        embedding = torch.empty((size,1), requires_grad=True,device=model_CFG.device)\n",
    "        nn.init.kaiming_uniform_(embedding, a=math.sqrt(5))  # or use kaiming_normal_\n",
    "        return embedding\n",
    "\n",
    "    def forward(self,X):\n",
    "        embeddings = []\n",
    "        for key,item in X.items():\n",
    "            # try:\n",
    "                if key in pragmas.categorical_space and item in ['off','flatten','']:\n",
    "                    embeddings.append(self.embeddings[f'{key}_{item}'])\n",
    "                    \n",
    "                elif key in pragmas.integer_space:\n",
    "                    value_embedding = self.reg_value_encoder[key](torch.tensor(item,dtype=torch.float32,device=model_CFG.device).reshape(1,1))\n",
    "\n",
    "                    embeddings.append(\n",
    "                        torch.vstack([self.embeddings[key],value_embedding.reshape(-1,1)])\n",
    "                    )\n",
    "                    \n",
    "                else:\n",
    "                    print(f\"Unknown Target {key} value: {item}\")\n",
    "                    raise f\"Unknown Target {key} value: {item}\"\n",
    "                    \n",
    "            # except Exception as ex:\n",
    "            #     print(ex)\n",
    "            #     print(f\"Unknown Target {key} value: {item}\")\n",
    "# #                 raise f\"Unknown Target {key} value: {item}\"\n",
    "\n",
    "        embeddings = torch.stack(embeddings,dim=0).reshape(1,-1,model_CFG.embed_size).to(device=model_CFG.device)\n",
    "        enc_out = self.enc(embeddings)\n",
    "        dec_out = self.dec(enc_out)\n",
    "\n",
    "        pred = torch.squeeze(dec_out, 0)\n",
    "        pred = pred.reshape(-1)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop:   0%|          | 1/33276 [00:00<1:10:35,  7.86it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop:  76%|███████▌  | 25370/33276 [08:32<02:39, 49.53it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m batch_x \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     16\u001b[0m batch_y \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 18\u001b[0m ypred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m target_classification \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(batch_y[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m'\u001b[39m],dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint64,device\u001b[38;5;241m=\u001b[39mmodel_CFG\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     20\u001b[0m target_ls \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mtensor(v,dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32) \u001b[38;5;28;01mfor\u001b[39;00m k,v \u001b[38;5;129;01min\u001b[39;00m batch_y\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k\u001b[38;5;241m!=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[54], line 59\u001b[0m, in \u001b[0;36mset_transformer.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[38;5;66;03m# except Exception as ex:\u001b[39;00m\n\u001b[1;32m     54\u001b[0m             \u001b[38;5;66;03m#     print(ex)\u001b[39;00m\n\u001b[1;32m     55\u001b[0m             \u001b[38;5;66;03m#     print(f\"Unknown Target {key} value: {item}\")\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# #                 raise f\"Unknown Target {key} value: {item}\"\u001b[39;00m\n\u001b[1;32m     58\u001b[0m         embeddings \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(embeddings,dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,model_CFG\u001b[38;5;241m.\u001b[39membed_size)\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mmodel_CFG\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 59\u001b[0m         enc_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menc\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m         dec_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdec(enc_out)\n\u001b[1;32m     62\u001b[0m         pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msqueeze(dec_out, \u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/conda/lib/python3.12/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/gitrepo/set_transformer/modules.py:41\u001b[0m, in \u001b[0;36mSAB.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m---> 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/gitrepo/set_transformer/modules.py:24\u001b[0m, in \u001b[0;36mMAB.forward\u001b[0;34m(self, Q, K)\u001b[0m\n\u001b[1;32m     21\u001b[0m K, V \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_k(K), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_v(K)\n\u001b[1;32m     23\u001b[0m dim_split \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim_V \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads\n\u001b[0;32m---> 24\u001b[0m Q_ \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(\u001b[43mQ\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim_split\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     25\u001b[0m K_ \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(K\u001b[38;5;241m.\u001b[39msplit(dim_split, \u001b[38;5;241m2\u001b[39m), \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     26\u001b[0m V_ \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(V\u001b[38;5;241m.\u001b[39msplit(dim_split, \u001b[38;5;241m2\u001b[39m), \u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/conda/lib/python3.12/site-packages/torch/_tensor.py:915\u001b[0m, in \u001b[0;36mTensor.split\u001b[0;34m(self, split_size, dim)\u001b[0m\n\u001b[1;32m    912\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(split_size, (\u001b[38;5;28mint\u001b[39m, torch\u001b[38;5;241m.\u001b[39mSymInt)):\n\u001b[0;32m--> 915\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m(\u001b[38;5;28mself\u001b[39m, split_size, dim)  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_VF\u001b[38;5;241m.\u001b[39msplit_with_sizes(\u001b[38;5;28mself\u001b[39m, split_size, dim)\n",
      "File \u001b[0;32m~/conda/lib/python3.12/site-packages/torch/_VF.py:26\u001b[0m, in \u001b[0;36mVFModule.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(name)\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvf \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_VariableFunctions\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr):\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvf, attr)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = set_transformer()\n",
    "\n",
    "clas_loss_fn = nn.CrossEntropyLoss()\n",
    "reg_loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=model_CFG.lr)\n",
    "\n",
    "model = model.to(device=model_CFG.device)\n",
    "train_epochs = []\n",
    "val_epochs = []\n",
    "for epoch in range(model_CFG.n_epochs):\n",
    "    print(f\"Training on {epoch}\")\n",
    "    train_losses = []\n",
    "    train_cl_losses = []\n",
    "    train_rg_losses = []\n",
    "    for i,batch in tqdm(enumerate(train_dataloader),total=len(train_dataloader),desc='Training Loop'):\n",
    "        optimizer.zero_grad()\n",
    "        batch_x = batch['X'][0]\n",
    "        batch_y = batch['y'][0]\n",
    "        \n",
    "        ypred = model(batch_x)\n",
    "        target_classification = torch.tensor(batch_y['valid'],dtype=torch.int64,device=model_CFG.device)\n",
    "        target_ls = [torch.tensor(v,dtype=torch.float32) for k,v in batch_y.items() if k!='valid']\n",
    "        target_regression = torch.tensor(target_ls,device=model_CFG.device)\n",
    "        \n",
    "        classification_loss = clas_loss_fn(ypred[:2],target_classification)\n",
    "        reggression_loss = reg_loss_fn(ypred[2:],target_regression)\n",
    "        \n",
    "\n",
    "        \n",
    "        total_loss = model_CFG.regression_weight * reggression_loss + classification_loss\n",
    "        # print(classification_loss,reggression_loss,total_loss)\n",
    "        \n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss = total_loss.detach().cpu()\n",
    "        classification_loss = classification_loss.detach().cpu()\n",
    "        reggression_loss = reggression_loss.detach().cpu()\n",
    "        train_losses.append(total_loss)\n",
    "        train_cl_losses.append(classification_loss)\n",
    "        train_rg_losses.append(reggression_loss)\n",
    "    train_epochs.append((train_losses,train_cl_losses,train_rg_losses))\n",
    "    print('Evaluation:-')\n",
    "    val_losses = []\n",
    "    val_cl_losses = []\n",
    "    val_rg_losses = []\n",
    "    for i,batch in tqdm(enumerate(val_dataloader),total=len(val_dataloader),desc='Training Loop'):\n",
    "\n",
    "        target_classification = torch.tensor(batch_y['valid'],dtype=torch.int64)\n",
    "        target_ls = [torch.tensor(v,dtype=torch.float32) for k,v in batch_y.items() if k!='valid']\n",
    "        target_regression = torch.tensor(target_ls,device=model_CFG.device)\n",
    "        \n",
    "        batch_x = batch['X'][0]\n",
    "        batch_y = batch['y'][0]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            ypred = model(batch_x)\n",
    "        \n",
    "            classification_loss = clas_loss_fn(ypred[:2],target_classification)\n",
    "            reggression_loss = reg_loss_fn(ypred[2:],target_regression)\n",
    "        \n",
    "            total_loss = model_CFG.regression_weight * reggression_loss + classification_loss\n",
    "            #    print(classification_loss,reggression_loss,total_loss)\n",
    "        \n",
    "            total_loss = total_loss.detach().cpu()\n",
    "            classification_loss = classification_loss.detach().cpu()\n",
    "            reggression_loss = reggression_loss.detach().cpu()\n",
    "            val_losses.append(total_loss)\n",
    "            val_cl_losses.append(classification_loss)\n",
    "            val_rg_losses.append(reggression_loss)\n",
    "        # break\n",
    "    val_epochs.append((val_losses,val_cl_losses,val_rg_losses))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mrg_losses\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "print(rg_losses[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5143, tensor(8.4839e+15))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(enumerate(rg_losses),key=lambda x:x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'X': {'design': {'__PARA__L0': 1,\n",
       "   '__PARA__L1': 1,\n",
       "   '__PARA__L2': 32,\n",
       "   '__PIPE__L0': 'off',\n",
       "   '__PIPE__L1': 'off',\n",
       "   '__TILE__L0': 1,\n",
       "   '__TILE__L1': 8},\n",
       "  'version': 'v18',\n",
       "  'graph': <networkx.classes.digraph.DiGraph at 0x72da448a98b0>,\n",
       "  'kernel_name': 'seidel-2d',\n",
       "  'src_code': '#pragma ACCEL kernel\\n\\nvoid kernel_seidel_2d(int tsteps,int n,double A[120][120])\\n{\\n  int t;\\n  int i;\\n  int j;\\n//#pragma scop\\n  \\n#pragma ACCEL PIPELINE auto{__PIPE__L0}\\n  \\n#pragma ACCEL TILE FACTOR=auto{__TILE__L0}\\n  \\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L0}\\n  for (t = 0; t <= 39; t++) {\\n    \\n#pragma ACCEL PIPELINE auto{__PIPE__L1}\\n    \\n#pragma ACCEL TILE FACTOR=auto{__TILE__L1}\\n    \\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L1}\\n    for (i = 1; i <= 118; i++) {\\n      \\n#pragma ACCEL PARALLEL FACTOR=auto{__PARA__L2}\\n      for (j = 1; j <= 118; j++) {\\n        A[i][j] = (A[i - 1][j - 1] + A[i - 1][j] + A[i - 1][j + 1] + A[i][j - 1] + A[i][j] + A[i][j + 1] + A[i + 1][j - 1] + A[i + 1][j] + A[i + 1][j + 1]) / 9.0;\\n      }\\n    }\\n  }\\n//#pragma endscop\\n}\\n'},\n",
       " 'y': {'valid': False,\n",
       "  'perf': 0.0,\n",
       "  'util-BRAM': 0,\n",
       "  'util-LUT': 0,\n",
       "  'util-FF': 0,\n",
       "  'util-DSP': 0}}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[5143]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x72d97002f530>]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.plot(range(len(cl_losses)),cl_losses,color='g')\n",
    "# plt.plot(range(len(rg_losses)),rg_losses,color='r')\n",
    "\n",
    "# plt.ylim([0,100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
